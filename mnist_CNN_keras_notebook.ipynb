{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest CNN for MNIST Digit-Rcognizer\n",
    "\n",
    "What is MNIST Dataset?\n",
    "MNIST Database is a collection of Images of Hand written Numbers (0-9). Each image is of size 28x28 pixels i.e 28 pixels in length and 28 pixels in height with only 1 axis to represent color gradient which is Black and White. Original Training set have 60,000 images for training and 10,000 images for test. But here in Kaggle we are given 42,000 images with label which will be used for training and 28,000 images to test/ predict results on.\n",
    "\n",
    "Entire Notebook has 4 major segments:\n",
    "1st - DATA EXTRACTION\n",
    "2ND - DATA PREPROCESSING\n",
    "3RD - MODELLING\n",
    "4TH - DATA POST PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/digit-recognizer/train.csv\n",
      "/kaggle/input/digit-recognizer/sample_submission.csv\n",
      "/kaggle/input/digit-recognizer/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXTRACTION\n",
    "Data extraction from Given CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_x_orig = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n",
    "test_x_orig = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n",
    "\n",
    "print(train_x_orig.shape)\n",
    "print(test_x_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data:\n",
    "train.csv and test.csv have Different shapes as Train.csv contains one extra column for actual labels of images 785-784. and 42,000 , 28,000 represent the number of images.\n",
    "\n",
    "The difference between them can be seen by calling there HEAD for 5 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traiing dataset columns    label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "\n",
      " test set columns    pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print('traiing dataset columns',train_x_orig.head())\n",
    "print('\\n test set columns',test_x_orig.head())\n",
    "\n",
    "print(type(train_x_orig))\n",
    "print(type(test_x_orig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Lables and pixels from our train_x_orig dataset.\n",
    "As we will preprocess the pixels values before fitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA extracted is in DATAFRAME FORMAT as seen by 'type(test_x_orig)' command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label_train  (42000, 1)\n",
      "\n",
      " Shape of pixels train (42000, 784)\n",
      "\n",
      " First 10 values of Labels_train [[1]\n",
      " [0]\n",
      " [1]\n",
      " [4]\n",
      " [0]\n",
      " [0]\n",
      " [7]\n",
      " [3]\n",
      " [5]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "labels_train = (train_x_orig['label'].values).reshape(-1,1)\n",
    "\n",
    "pixels_train = train_x_orig.drop('label', axis = 1)\n",
    "\n",
    "print('Shape of label_train ',labels_train.shape)\n",
    "print('\\n Shape of pixels train',pixels_train.shape)\n",
    "\n",
    "print('\\n First 10 values of Labels_train',labels_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above steps shows how we have seperated Labels and Pixels from our original Dataset for all 42,000 images.\n",
    "\n",
    "Next Step is DATA PREPROCESSING."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "Why we need DATA preprocessing?\n",
    "\n",
    "![](https://ibb.co/JtLdz6f)\n",
    "As its clearly visible from the Image above, the pixels values follows 8-bit color system where maximum number for a color shade can be 256. including zero the scale ranges from 0-255 , 0 being full black and 255 being full white. Numbers in between are just shades of White and grey.\n",
    "\n",
    "Lets Visualize one Image from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE DATA TYPE  <class 'numpy.ndarray'>\n",
      "\n",
      " shape of 2nd image pixels =  (784,)\n"
     ]
    }
   ],
   "source": [
    "# 1st to convert DATAFRAME to uint8 datatype as array\n",
    "\n",
    "im = np.array(pixels_train, dtype = 'uint8')\n",
    "print('IMAGE DATA TYPE ',type(im))\n",
    "\n",
    "# taking 2nd image from dataset of 42,000, python 0 = real world 1, python 1 = real world 2\n",
    "\n",
    "im1 = im[1] # changing [this number] for im will give you any index of image you want to visualize\n",
    "print('\\n shape of 2nd image pixels = ', im1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the 2nd image seperated is having length of 784 and plotting that would'nt make any sense, so we reshape it as 28x28 which is our image Dimension 28 x 28 = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fec465089e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im1 = im1.reshape((28,28))\n",
    "plt.imshow(im1, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Label for correspoing image from Labels_train dataset = labels_train[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(labels_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing further we would be diving our dataset into 3 groups \n",
    "\n",
    "1- Training data- model will train itself on this data.\n",
    "\n",
    "2- Validation Data- model will only checks its validity on this data, \n",
    "   no direct training is done on this dataset.\n",
    "   \n",
    "3- Test dataset- this is being provided for aking predictions on. this dataset doesnt have labels ,      it only have pixel values which our model will provide labels for once trained. \n",
    "\n",
    "ratio between Train / validation dataset will be 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pix_train, pix_valid, label_train, label_valid = train_test_split(pixels_train, labels_train, test_size = 0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data type for pix_train =  <class 'pandas.core.frame.DataFrame'>\n",
      " Data type for label_train =  <class 'numpy.ndarray'>\n",
      " Data type for pix_valid =  <class 'pandas.core.frame.DataFrame'>\n",
      " Data type for label_valid =  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# data types of all variables\n",
    "print(' Data type for pix_train = ',type(pix_train))\n",
    "print(' Data type for label_train = ',type(label_train))\n",
    "print(' Data type for pix_valid = ',type(pix_valid))\n",
    "print(' Data type for label_valid = ',type(label_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable names can be confusing at times but upon reading and writing the code 2-3 times it becomes more clear:\n",
    "\n",
    "pix_train ::: pixels for training set\n",
    "\n",
    "labels_train ::: Labels for training set \n",
    "\n",
    "pix_valid ::: pixels for validation set\n",
    "\n",
    "label_valid ::: labels for validation set\n",
    "\n",
    "Also pixels_data is in still DATAFRAME format, we will conver this into 'float32' for mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data type for pix_train =  <class 'numpy.ndarray'>\n",
      " Data type for pix_valid =  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "pix_train = (pix_train.values).astype('float32')\n",
    "pix_test = (test_x_orig.values).astype('float32')\n",
    "pix_valid = (pix_valid.values).astype('float32')\n",
    "\n",
    "\n",
    "print(' Data type for pix_train = ',type(pix_train))\n",
    "print(' Data type for pix_valid = ',type(pix_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3 set of pixel data stored in pix_train, pix_valid, and pix_test.\n",
    "\n",
    "as the color encoding of images are in 8 bit they have maximum values of 255 to represent White color. We will be scaling this data by diving all the pixel values by 255.0.\n",
    "\n",
    "This step will ensure none of the pixel value is greater than 1.0 and less then 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value in pix_train = 1.0\n",
      "Minimum value in pix_train = 0.0\n"
     ]
    }
   ],
   "source": [
    "pix_train /= 255.0\n",
    "pix_test  /= 255.0\n",
    "pix_valid /= 255.0\n",
    "\n",
    "print('Maximum value in pix_train =', np.max(pix_test))\n",
    "print('Minimum value in pix_train =', np.min(pix_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Reshaping: in order to fit our data into the conv model we must reshape our data as:\n",
    "\n",
    "(number of images, 28,28,1)\n",
    "\n",
    "currently our data is shaped as (number of images,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pix_train = (29400, 784)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of pix_train =',pix_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pix_train after reshaping= (29400, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshaping\n",
    "pix_train = pix_train.reshape(pix_train.shape[0], 28,28,1)\n",
    "pix_test = pix_test.reshape(pix_test.shape[0], 28, 28,1)\n",
    "pix_valid = pix_valid.reshape(pix_valid.shape[0], 28, 28,1)\n",
    "\n",
    "print('Shape of pix_train after reshaping=',pix_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing LABELS\n",
    "\n",
    "till now we were busy to make our pixel data ready for our model.\n",
    "\n",
    "Now we need to process our labels, here we will use ONE HOT ENCODING.\n",
    "\n",
    "the best article for understanding ONE HOT ENCODING - [https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/](httpS://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)\n",
    "\n",
    "Our Digits/ labels ranges from 0-9 i.e 10 classes, it is required to input this number in our model last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes 10\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.unique(labels_train))\n",
    "\n",
    "print('Number of classes', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding will categorise our labels into 0-1 instead of 0-9, to represent number between 0-9 using numbers 0-9 is easy. \n",
    "\n",
    "Example: 9 is a 9 and we understand it as 9.\n",
    "but our model require more simple version of this and the only vocabulary it has of digits are 0,1.\n",
    "\n",
    "we have to represent number 9 in form of 0 and 1.\n",
    "\n",
    "this is done via hot hot encoding which do the following conversion to entire labele_data.\n",
    "\n",
    "0 = [1, 0, 0, 0, 0, 0, 0 , 0, 0, 0]\n",
    "\n",
    "1 = [0, 1, 0, 0, 0, 0, 0 , 0, 0, 0]\n",
    "\n",
    "2 = [1, 0, 1, 0, 0, 0, 0 , 0, 0, 0]\n",
    "\n",
    "3 = [1, 0, 0, 1, 0, 0, 0 , 0, 0, 0]\n",
    "\n",
    "4 = [1, 0, 0, 0, 1, 0, 0 , 0, 0, 0]\n",
    "\n",
    "5 = [1, 0, 0, 0, 0, 1, 0 , 0, 0, 0]\n",
    "\n",
    "6 = [1, 0, 0, 0, 0, 0, 1 , 0, 0, 0]\n",
    "\n",
    "7 = [1, 0, 0, 0, 0, 0, 0 , 1, 0, 0]\n",
    "\n",
    "8 = [1, 0, 0, 0, 0, 0, 0 , 0, 1, 0]\n",
    "\n",
    "9 = [1, 0, 0, 0, 0, 0, 0 , 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after one hot encoder [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# using ONE HOT ENCODER\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "label_train = to_categorical(label_train)\n",
    "label_valid = to_categorical(label_valid)\n",
    "\n",
    "print('after one hot encoder',label_valid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can manually count the number and say the number represented by this vector is 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLING\n",
    "\n",
    "Keras Convolution Model\n",
    "\n",
    "Since this is the core of this code I assume that you know the basics.\n",
    "\n",
    "Some of the important terms are: PADDING, KERNEL_SIZE, STRIDE, number of neurons in any dimension\n",
    "\n",
    "[assuming dimension in both Height, width are equal]\n",
    "\n",
    "formula to calculate number of pixels after 1 conv layer is applied.\n",
    "\n",
    "number of pixel in next layer = [(number of pixel in last layer + 2 x padding - kernel_size) / stride] + 1\n",
    "\n",
    "\n",
    "for our model we will follow the below mentioned model structure::\n",
    "\n",
    "1st conv layer : number of filter 64, kernel_size = (3,3), strides = 1, padding = 0\n",
    "\n",
    "2nd conv layer : number of filter 64, kernel_size = (3,3), strides = 1, padding = 0\n",
    "\n",
    "1st max Pooling layer : kernel_size = (2,2)\n",
    "\n",
    "3rd conv layer : number of filter 128, kernel_size = (3,3), strides = 1, padding = 0\n",
    "\n",
    "4th conv layer : number of filter 128, kernel_size = (3,3), strides = 1, padding = 0\n",
    "\n",
    "2nd max Pooling layer : kernel_size = (2,2)\n",
    "\n",
    "####################################################################\n",
    "\n",
    "using formula to get output dim for 1st conv layer:\n",
    "\n",
    "padding = 0 = padding = 'valid'\n",
    "\n",
    "1st Conv layer output = [(28 + 0 - 3) / 1] + 1  =  26\n",
    "\n",
    "1st Conv layer output = 26 x 26 x 64\n",
    "\n",
    "2nd Conv layer output = [(26 + 0 - 3)  /  1] + 1 = 24\n",
    "\n",
    "2nd Conv layer output = 24 x 24 x 64\n",
    "\n",
    "1st Max Pool layer output = 24 x 24 x 64 -->> 12 x 12 x 64\n",
    "\n",
    "3rd Conv layer output = [(12 + 0 -3)  / 1] + 1  = 10\n",
    "\n",
    "3rd Conv layer output = 10 x 10 x 128\n",
    "\n",
    "4th Conv layer output = [(10 + 0 -3)  / 1] + 1 = 8\n",
    "\n",
    "4th Conv layer output = 8 x 8 x 128\n",
    "\n",
    "2nd Max Pool layer output = 8 x 8 x 64 -->> 4 x 4 x 128\n",
    "\n",
    "\n",
    "\n",
    "Now as our Image has reduced to 4 x 4 x 128 we will now be getting back to traditional Neural network,\n",
    "by using 4 x 4 x 64 neurons all in one layer, instead of 4 x 4 in 128.\n",
    "\n",
    "Total number of neurons in layer = 4 x 4 x 128 = 2048\n",
    "\n",
    "we will be having 2 layers of 256 nuerons and last output layer of 10 neurons for 10 classes that we have.\n",
    "\n",
    "2048 --> 512 --> 256 --> 10\n",
    "\n",
    "\n",
    "Activations Functions used - ReLu for all layers except last,\n",
    "It will have Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, BatchNormalization, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'valid', activation = 'relu', input_shape = pix_train.shape[1:] ))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'SGD', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is now compiled we can match our calculated dimensions with the models structure.\n",
    "Use model.summary() to get  the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,445,066\n",
      "Trainable params: 1,443,530\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last step for Modelling, here we will fit the model with the data we have pre-processed and pass on the number of epochs, batch_size and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29400 samples, validate on 12600 samples\n",
      "Epoch 1/100\n",
      "29400/29400 [==============================] - 8s 273us/step - loss: 0.4134 - acc: 0.8742 - val_loss: 0.1438 - val_acc: 0.9582\n",
      "Epoch 2/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.1418 - acc: 0.9592 - val_loss: 0.0829 - val_acc: 0.9760\n",
      "Epoch 3/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.1056 - acc: 0.9690 - val_loss: 0.0746 - val_acc: 0.9775\n",
      "Epoch 4/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0802 - acc: 0.9756 - val_loss: 0.0658 - val_acc: 0.9792\n",
      "Epoch 5/100\n",
      "29400/29400 [==============================] - 5s 163us/step - loss: 0.0720 - acc: 0.9787 - val_loss: 0.0507 - val_acc: 0.9840\n",
      "Epoch 6/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0627 - acc: 0.9820 - val_loss: 0.0482 - val_acc: 0.9856\n",
      "Epoch 7/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0548 - acc: 0.9835 - val_loss: 0.0442 - val_acc: 0.9863\n",
      "Epoch 8/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0475 - acc: 0.9859 - val_loss: 0.0589 - val_acc: 0.9820\n",
      "Epoch 9/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0438 - acc: 0.9873 - val_loss: 0.0412 - val_acc: 0.9870\n",
      "Epoch 10/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0409 - acc: 0.9877 - val_loss: 0.0491 - val_acc: 0.9843\n",
      "Epoch 11/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0378 - acc: 0.9888 - val_loss: 0.0432 - val_acc: 0.9863\n",
      "Epoch 12/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0350 - acc: 0.9903 - val_loss: 0.0394 - val_acc: 0.9869\n",
      "Epoch 13/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0312 - acc: 0.9910 - val_loss: 0.0638 - val_acc: 0.9810\n",
      "Epoch 14/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0317 - acc: 0.9908 - val_loss: 0.0398 - val_acc: 0.9884\n",
      "Epoch 15/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0293 - acc: 0.9913 - val_loss: 0.0421 - val_acc: 0.9881\n",
      "Epoch 16/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0256 - acc: 0.9927 - val_loss: 0.0411 - val_acc: 0.9874\n",
      "Epoch 17/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0236 - acc: 0.9932 - val_loss: 0.0395 - val_acc: 0.9887\n",
      "Epoch 18/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0217 - acc: 0.9940 - val_loss: 0.0327 - val_acc: 0.9901\n",
      "Epoch 19/100\n",
      "29400/29400 [==============================] - 5s 168us/step - loss: 0.0207 - acc: 0.9946 - val_loss: 0.0336 - val_acc: 0.9902\n",
      "Epoch 20/100\n",
      "29400/29400 [==============================] - 5s 169us/step - loss: 0.0186 - acc: 0.9949 - val_loss: 0.0329 - val_acc: 0.9901\n",
      "Epoch 21/100\n",
      "29400/29400 [==============================] - 5s 163us/step - loss: 0.0177 - acc: 0.9954 - val_loss: 0.0307 - val_acc: 0.9902\n",
      "Epoch 22/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0170 - acc: 0.9949 - val_loss: 0.0323 - val_acc: 0.9901\n",
      "Epoch 23/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0157 - acc: 0.9957 - val_loss: 0.0312 - val_acc: 0.9909\n",
      "Epoch 24/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0156 - acc: 0.9954 - val_loss: 0.0323 - val_acc: 0.9900\n",
      "Epoch 25/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0150 - acc: 0.9960 - val_loss: 0.0323 - val_acc: 0.9902\n",
      "Epoch 26/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0145 - acc: 0.9959 - val_loss: 0.0327 - val_acc: 0.9910\n",
      "Epoch 27/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0133 - acc: 0.9963 - val_loss: 0.0318 - val_acc: 0.9911\n",
      "Epoch 28/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0118 - acc: 0.9969 - val_loss: 0.0319 - val_acc: 0.9906\n",
      "Epoch 29/100\n",
      "29400/29400 [==============================] - 5s 163us/step - loss: 0.0114 - acc: 0.9971 - val_loss: 0.0356 - val_acc: 0.9887\n",
      "Epoch 30/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0114 - acc: 0.9974 - val_loss: 0.0333 - val_acc: 0.9910\n",
      "Epoch 31/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0097 - acc: 0.9977 - val_loss: 0.0305 - val_acc: 0.9915\n",
      "Epoch 32/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0100 - acc: 0.9974 - val_loss: 0.0300 - val_acc: 0.9911\n",
      "Epoch 33/100\n",
      "29400/29400 [==============================] - 5s 166us/step - loss: 0.0103 - acc: 0.9974 - val_loss: 0.0293 - val_acc: 0.9915\n",
      "Epoch 34/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0087 - acc: 0.9979 - val_loss: 0.0289 - val_acc: 0.9919\n",
      "Epoch 35/100\n",
      "29400/29400 [==============================] - 5s 163us/step - loss: 0.0083 - acc: 0.9980 - val_loss: 0.0305 - val_acc: 0.9916\n",
      "Epoch 36/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0084 - acc: 0.9979 - val_loss: 0.0296 - val_acc: 0.9916\n",
      "Epoch 37/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0078 - acc: 0.9981 - val_loss: 0.0323 - val_acc: 0.9915\n",
      "Epoch 38/100\n",
      "29400/29400 [==============================] - 5s 163us/step - loss: 0.0079 - acc: 0.9981 - val_loss: 0.0297 - val_acc: 0.9913\n",
      "Epoch 39/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0293 - val_acc: 0.9909\n",
      "Epoch 40/100\n",
      "29400/29400 [==============================] - 5s 163us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0317 - val_acc: 0.9906\n",
      "Epoch 41/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0073 - acc: 0.9981 - val_loss: 0.0301 - val_acc: 0.9922\n",
      "Epoch 42/100\n",
      "29400/29400 [==============================] - 5s 163us/step - loss: 0.0063 - acc: 0.9987 - val_loss: 0.0294 - val_acc: 0.9911\n",
      "Epoch 43/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0067 - acc: 0.9984 - val_loss: 0.0291 - val_acc: 0.9918\n",
      "Epoch 44/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.0304 - val_acc: 0.9914\n",
      "Epoch 45/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0051 - acc: 0.9991 - val_loss: 0.0360 - val_acc: 0.9895\n",
      "Epoch 46/100\n",
      "29400/29400 [==============================] - 5s 166us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.0348 - val_acc: 0.9911\n",
      "Epoch 47/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.0288 - val_acc: 0.9921\n",
      "Epoch 48/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0051 - acc: 0.9989 - val_loss: 0.0308 - val_acc: 0.9907\n",
      "Epoch 49/100\n",
      "29400/29400 [==============================] - 5s 184us/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.0308 - val_acc: 0.9917\n",
      "Epoch 50/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0048 - acc: 0.9992 - val_loss: 0.0290 - val_acc: 0.9921\n",
      "Epoch 51/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 0.0314 - val_acc: 0.9915\n",
      "Epoch 52/100\n",
      "29400/29400 [==============================] - 5s 163us/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.0299 - val_acc: 0.9927\n",
      "Epoch 53/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0048 - acc: 0.9989 - val_loss: 0.0304 - val_acc: 0.9927\n",
      "Epoch 54/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0295 - val_acc: 0.9926\n",
      "Epoch 55/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0044 - acc: 0.9991 - val_loss: 0.0373 - val_acc: 0.9901\n",
      "Epoch 56/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 0.0305 - val_acc: 0.9919\n",
      "Epoch 57/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0043 - acc: 0.9993 - val_loss: 0.0302 - val_acc: 0.9922\n",
      "Epoch 58/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0033 - acc: 0.9995 - val_loss: 0.0308 - val_acc: 0.9919\n",
      "Epoch 59/100\n",
      "29400/29400 [==============================] - 5s 163us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.0397 - val_acc: 0.9902\n",
      "Epoch 60/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0039 - acc: 0.9993 - val_loss: 0.0292 - val_acc: 0.9924\n",
      "Epoch 61/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.0299 - val_acc: 0.9921\n",
      "Epoch 62/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.0311 - val_acc: 0.9921\n",
      "Epoch 63/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 0.0288 - val_acc: 0.9924\n",
      "Epoch 64/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.0305 - val_acc: 0.9923\n",
      "Epoch 65/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0032 - acc: 0.9996 - val_loss: 0.0291 - val_acc: 0.9926\n",
      "Epoch 66/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0297 - val_acc: 0.9921\n",
      "Epoch 67/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0030 - acc: 0.9995 - val_loss: 0.0283 - val_acc: 0.9929\n",
      "Epoch 68/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.0293 - val_acc: 0.9925\n",
      "Epoch 69/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.0327 - val_acc: 0.9915\n",
      "Epoch 70/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0292 - val_acc: 0.9918\n",
      "Epoch 71/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0305 - val_acc: 0.9921\n",
      "Epoch 72/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.0300 - val_acc: 0.9930\n",
      "Epoch 73/100\n",
      "29400/29400 [==============================] - 5s 165us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0299 - val_acc: 0.9925\n",
      "Epoch 74/100\n",
      "29400/29400 [==============================] - 5s 164us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0306 - val_acc: 0.9919\n",
      "Epoch 75/100\n",
      "22400/29400 [=====================>........] - ETA: 1s - loss: 0.0029 - acc: 0.9994"
     ]
    }
   ],
   "source": [
    "hist = model.fit(pix_train, label_train, epochs = 100, batch_size = 64, validation_data = (pix_valid, label_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of epochs can be changed to understand the learning curve of the model.\n",
    "Parameters that can regulated by users are:\n",
    "\n",
    "[PADDING, KERNEL_SIZE, STRIDE, POOLING](https://keras.io/layers/convolutional/), [NUMBER OF NEURON IN DENSE LAYER(256 --> 1024, ANYTHING)](https://keras.io/layers/core/#dense), [ACTIVATION FUNCTIONS](https://keras.io/activations/), [OPTIMIZERS](https://keras.io/optimizers/),[ LOSS FUNCTIONS](https://keras.io/losses/), [EPOCHS, BATCH-SIZE.](https://keras.io/getting-started/faq/#what-does-sample-batch-epoch-mean)\n",
    "\n",
    "\n",
    "plotting the Loss and Accuracy curve with number of epochs on y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss during training\n",
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(hist.history['acc'], label='train')\n",
    "plt.plot(hist.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA POST PROCESSING\n",
    "\n",
    "As our model is now trained we will use it for predicting labels for TEST set provided and then converting that output to the format required by KAGGLE to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(pix_test)\n",
    "\n",
    "results = np.argmax(results, axis = 1)\n",
    "\n",
    "results = pd.Series(results, name = 'Label')\n",
    "\n",
    "sub = pd.concat([pd.Series(range(1,28001), name = 'ImageId'), results], axis = 1)\n",
    "\n",
    "sub.to_csv('csv_to_submit.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our result file will be saved names as fin_01.csv.\n",
    "\n",
    "There are Lot more options avalable to tweak with CONV neural Network,\n",
    "like [xavier initialization](https://keras.io/initializers/), [Dropout](https://keras.io/layers/core/#dropout), [BatchNormalization](https://keras.io/backend/#batch_normalization), [ImagedataGenerator](https://keras.io/preprocessing/image/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
